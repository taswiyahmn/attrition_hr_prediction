# -*- coding: utf-8 -*-
"""prediksi_attrition_perusahaan_HR

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mIkyTVvWQZRJiJeB_cHInu2PSqVwsvKj

# **Import Library**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import shap
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, FunctionTransformer
import joblib

"""# **Load Data**"""

!wget https://raw.githubusercontent.com/dicodingacademy/dicoding_dataset/refs/heads/main/employee/employee_data.csv
# 2. Load data dari raw GitHub URL
df = pd.read_csv('employee_data.csv')

# 3. Cek 5 baris awal
df.head()

"""Kode ini digunakan untuk mengunduh dan memuat dataset karyawan dari repositori GitHub milik Dicoding. File CSV diunduh menggunakan perintah wget, kemudian dibaca ke dalam sebuah DataFrame menggunakan pandas.read_csv(). Setelah data berhasil dimuat, fungsi df.head() digunakan untuk menampilkan lima baris pertama dari dataset, guna melakukan inspeksi awal terhadap struktur data dan isi kolom. Langkah ini penting sebagai bagian dari proses eksplorasi data sebelum dilakukan analisis lebih lanjut.

# **EDA**
"""

df.describe()

"""Berdasarkan hasil analisis deskriptif, terdapat beberapa kolom seperti Age, DistanceFromHome, dan lainnya yang secara statistik memungkinkan **adanya outlier**."""

df.info()

# Cek jumlah duplikat berdasarkan movie_id
print("Data duplikat pada employee data: ",df.duplicated().sum())
print("Data null pada employee data: ",df.isnull().sum())

"""Berdasarkan hasil pemeriksaan kualitas data,** tidak ditemukan adanya data duplikat dalam dataset**, yang menunjukkan bahwa setiap entri bersifat unik. Namun, ditemukan adanya** 412 nilai kosong (null)** pada kolom Attrition."""

df['Attrition'].value_counts()

"""Kolom Attrition merepresentasikan status karyawan, di mana nilai **0 menunjukkan karyawan yang tidak mengundurkan diri (aktif)**, sedangkan **nilai 1 menunjukkan karyawan yang telah mengundurkan diri (resign)**. Berdasarkan distribusi data, terdapat 879 karyawan aktif dan 179 karyawan yang telah resign. Informasi ini menunjukkan adanya ketidakseimbangan kelas yang perlu dipertimbangkan dalam proses pemodelan, terutama jika menggunakan algoritma yang sensitif terhadap distribusi target."""

df['Gender'].value_counts()

"""Berdasarkan hasil analisis terhadap kolom Gender, ditemukan bahwa jumlah karyawan laki-laki lebih banyak dibandingkan dengan karyawan perempuan."""

pd.crosstab(df['Department'], df['Attrition'])

"""Berdasarkan analisis tingkat attrition per departemen, ditemukan bahwa jumlah karyawan yang mengundurkan diri **paling sedikit berasal dari divisi Human Resources (HR)**, sementara **jumlah attrition tertinggi tercatat pada departemen Research and Development, yaitu sebanyak 107 karyawan.**"""

# Calculate the correlation matrix
correlation_matrix = df.corr(numeric_only=True)
sns.heatmap(correlation_matrix, cmap='coolwarm')

# Display the plot
plt.show()

"""Berdasarkan hasil visualisasi korelasi melalui heatmap, karena variabel target dalam analisis ini adalah Attrition, maka kolom-kolom seperti **EmployeeCount dan StandardHours** dianggap tidak relevan untuk pemodelan. Hal ini disebabkan oleh tidak adanya variasi nilai dalam kolom tersebut, sehingga tidak memberikan kontribusi informatif terhadap prediksi Attrition."""

sns.boxplot(x='Attrition', y='Age', data=df)
plt.title('Usia Karyawan berdasarkan Attrition')
plt.show()

"""Karyawan dengan status Attrition 0 (tidak mengundurkan diri) rata-rata berusia antara 30 hingga 40 tahun, sedangkan karyawan dengan status Attrition 1 (mengundurkan diri) umumnya berada pada rentang usia sekitar 28 hingga kurang dari 40 tahun"""

plt.figure(figsize=(8,6))
sns.violinplot(x='Attrition', y='YearsAtCompany', data=df)

plt.title('Distribution of Years at Company by Attrition')
plt.show()

"""+ Karyawan dengan Attrition 0 umumnya memiliki nilai antara lebih dari 0 hingga kurang dari 10.

+ Karyawan dengan Attrition 1 umumnya memiliki nilai kurang dari 5.
"""

# Hitung jumlah karyawan berdasarkan Department dan Attrition
attrition_counts = df.groupby(['Department', 'Attrition']).size().reset_index(name='Count')

# Ubah kolom Attrition menjadi label (opsional)
attrition_counts['Attrition'] = attrition_counts['Attrition'].map({0: 'Stay', 1: 'Left'})

# Buat bar chart dan simpan Axes object ke variabel ax
plt.figure(figsize=(10, 6))
ax = sns.barplot(data=attrition_counts, x='Department', y='Count', hue='Attrition')

# Tambahkan data label di atas bar
for container in ax.containers:
    ax.bar_label(container, label_type='edge', fontsize=10)

plt.title("Jumlah Karyawan yang Stay vs Left per Department")
plt.xlabel("Department")
plt.ylabel("Jumlah Karyawan")
plt.legend(title="Attrition Status")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Kode ini digunakan untuk menghitung dan memvisualisasikan jumlah karyawan yang tetap bekerja (Stay) dan yang keluar (Left) di setiap departemen. Pertama, data dikelompokkan berdasarkan kombinasi Department dan Attrition, kemudian dihitung jumlahnya menggunakan groupby dan size. Nilai Attrition yang semula berupa angka dikonversi menjadi label kategori untuk memperjelas visualisasi. Selanjutnya, dibuat bar chart menggunakan seaborn dengan parameter hue untuk membedakan status attrition. Label jumlah ditambahkan di atas masing-masing batang menggunakan bar_label untuk memperjelas informasi yang ditampilkan. Visualisasi ini memberikan gambaran yang jelas mengenai distribusi attrition per departemen, dengan tampilan yang rapi dan informatif."""

education_map = {
    1: 'Below College',
    2: 'College',
    3: 'Bachelor',
    4: 'Master',
    5: 'Doctor'
}

# Buat Series baru untuk visualisasi, tanpa menambah kolom baru ke df
education_labels = df['Education'].map(education_map)

# Hitung jumlah per kategori pendidikan
edu_counts = education_labels.value_counts().sort_index()

labels = edu_counts.index.tolist()
counts = edu_counts.values.tolist()

# Fungsi gabung persen dan jumlah
def make_autopct(values):
    def my_autopct(pct):
        total = sum(values)
        count = int(round(pct * total / 100.0))
        return f"{pct:.1f}%\n({count})"
    return my_autopct

# Buat pie chart
plt.figure(figsize=(7, 7))
plt.pie(counts, labels=labels, autopct=make_autopct(counts), startangle=90)

plt.title('Distribusi Pendidikan Karyawan')
plt.axis('equal')  # Supaya pie bulat
plt.show()

"""Distribusi tingkat pendidikan karyawan didominasi oleh lulusan Bachelor dengan jumlah 572 data, diikuti oleh lulusan Master sebanyak 398. Sementara itu, tingkat pendidikan dengan jumlah paling sedikit adalah Doctor, yaitu sebanyak 48 data."""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

plt.figure(figsize=(12,7))  # Tambah tinggi supaya muat tabel

# Job Satisfaction
plt.subplot(2, 2, 1)
sns.countplot(x='JobSatisfaction', data=df, palette='Blues')
plt.title('Distribusi Job Satisfaction')
plt.xlabel('Rating')
plt.ylabel('Jumlah Karyawan')

# Job Satisfaction summary table
job_counts = df['JobSatisfaction'].value_counts().sort_index()
plt.subplot(2, 2, 3)  # Bawah plot 1
plt.axis('off')  # Hide axis

# Buat tabel data
job_table = pd.DataFrame({'Rating': job_counts.index, 'Jumlah': job_counts.values})
table = plt.table(cellText=job_table.values,
                  colLabels=job_table.columns,
                  cellLoc='center',
                  loc='center')
table.auto_set_font_size(False)
table.set_fontsize(12)
table.scale(1, 2)  # Lebar, tinggi cell

plt.title('Tabel Jumlah Job Satisfaction')

# Work Life Balance
plt.subplot(2, 2, 2)
sns.countplot(x='WorkLifeBalance', data=df, palette='Greens')
plt.title('Distribusi Work Life Balance')
plt.xlabel('Rating')
plt.ylabel('Jumlah Karyawan')

# Work Life Balance summary table
wlb_counts = df['WorkLifeBalance'].value_counts().sort_index()
plt.subplot(2, 2, 4)  # Bawah plot 2
plt.axis('off')

wlb_table = pd.DataFrame({'Rating': wlb_counts.index, 'Jumlah': wlb_counts.values})
table = plt.table(cellText=wlb_table.values,
                  colLabels=wlb_table.columns,
                  cellLoc='center',
                  loc='center')
table.auto_set_font_size(False)
table.set_fontsize(12)
table.scale(1, 2)

plt.title('Tabel Jumlah Work Life Balance')

plt.tight_layout()
plt.show()

"""Berdasarkan distribusi data, mayoritas karyawan di perusahaan Jaya Jaya Maju memberikan penilaian pada tingkat 3 dan 4 untuk indikator Job Satisfaction. Pola serupa juga terlihat pada indikator Work Life Balance, yang juga didominasi oleh rating 3 dan 4. Hal ini mengindikasikan bahwa secara umum, karyawan merasa cukup puas terhadap pekerjaannya dan memiliki keseimbangan kehidupan kerja yang baik.

# **Data Cleaning**
"""

df_cleaned = df.copy()

# Data yang lengkap (punya label)
df_train = df_cleaned[df_cleaned['Attrition'].notnull()]
# Data tanpa label (kosong)
df_unlabeled = df_cleaned[df_cleaned['Attrition'].isnull()]

df_train.to_csv('attrition_terbaru.csv', index=False)

"""Dataset dibagi menjadi dua bagian untuk keperluan pemodelan dan interpretasi hasil:

### 1. `df_train`
- Dataset ini digunakan untuk pelatihan (training) model.
- Seluruh data pada dataset ini **tidak mengandung nilai null**, terutama pada kolom target `Attrition`.
- Digunakan sebagai data utama dalam proses pembelajaran mesin karena bersih dan lengkap.

### 2. `df_unlabeled`
- Dataset ini digunakan untuk proses interpretasi hasil model setelah pelatihan.
- Kolom `Attrition` pada dataset ini **bernilai null**, sehingga tidak digunakan dalam proses pelatihan.
- Setelah model dilatih menggunakan `df_train`, nilai `Attrition` pada dataset ini akan **diprediksi dan diisi oleh model**.

Dengan pembagian ini, proses pelatihan model menjadi lebih akurat dan hasil prediksi dapat diterapkan langsung pada data yang belum diketahui status `Attrition`-nya.

### **Handling Unrelevant Columns**
"""

df_train = df_train.drop(['EmployeeId', 'Over18', 'StandardHours'], axis=1)

"""Berdasarkan hasil visualisasi **heatmap korelasi**, dilakukan identifikasi terhadap kolom-kolom yang memiliki tingkat korelasi rendah terhadap variabel target `Attrition`. Kolom-kolom tersebut dianggap **tidak memiliki kontribusi signifikan** dalam proses prediksi.

### **Label Encoding**
"""

# 2. Label Encoding untuk kolom biner
df_train['Gender'] = df_train['Gender'].map({'Male': 0, 'Female': 1})
df_train['OverTime'] = df_train['OverTime'].map({'No': 0, 'Yes': 1})

# 3. One-Hot Encoding untuk kolom nominal
one_hot_columns = ['BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus']
df_train = pd.get_dummies(df_train, columns=one_hot_columns, drop_first=True)  # drop_first=True untuk menghindari dummy trap

"""Untuk mempersiapkan data sebelum proses pemodelan, dilakukan encoding pada fitur-fitur kategorikal sebagai berikut:

### 1. Encoding Label (Label Encoding)
- Fitur `Gender` dan `OverTime` memiliki dua kategori (biner), sehingga dilakukan proses encoding menggunakan **label encoding**.
- Tujuannya adalah untuk mengubah nilai kategorikal menjadi representasi numerik sederhana (misalnya: Laki-laki = 1, Perempuan = 0).

### 2. One-Hot Encoding
- Fitur `BusinessTravel`, `Department`, `EducationField`, `JobRole`, dan `MaritalStatus` memiliki lebih dari dua kategori.
- Oleh karena itu, kelima kolom ini diencode menggunakan **one-hot encoding**, yaitu mengubah setiap kategori unik menjadi kolom biner tersendiri.

# **Split Data**
"""

df_rf = df_train.copy()   # untuk random forest
df_knn = df_train.copy()  # untuk knn (scaling dilakukan di sini)

"""Agar proses pelatihan pada dua model berbeda tidak saling memengaruhi, dataset `df_train` diduplikasi menjadi dua salinan terpisah:

- **`df_rf`**: Salinan `df_train` yang akan digunakan khusus untuk pelatihan model **Random Forest**.
- **`df_knn`**: Salinan `df_train` yang akan digunakan khusus untuk pelatihan model **K-Nearest Neighbors (KNN)**.

"""

# Untuk Random Forest
X_rf = df_rf.drop('Attrition', axis=1)
y_rf = df_rf['Attrition']
X_rf_train, X_rf_test, y_rf_train, y_rf_test = train_test_split(X_rf, y_rf, test_size=0.2, random_state=42)

# Untuk KNN
X_knn = df_knn.drop('Attrition', axis=1)
y_knn = df_knn['Attrition']
X_knn_train, X_knn_test, y_knn_train, y_knn_test = train_test_split(X_knn, y_knn, test_size=0.2, random_state=42)

"""Sebelum melakukan pembagian data menjadi set pelatihan dan pengujian, kolom `Attrition` dipisahkan terlebih dahulu sebagai variabel target (`y`) untuk kedua dataset pemodelan, yaitu `df_rf` dan `df_knn`.

Langkah-langkahnya:
1. Pisahkan kolom `Attrition` dari fitur lainnya untuk mendapatkan variabel target (`y_rf` dan `y_knn`) dan fitur (`X_rf` dan `X_knn`).
2. Lakukan pembagian data menjadi data pelatihan dan pengujian (train-test split) secara terpisah untuk masing-masing dataset, agar kedua model dapat dilatih dan diuji secara independen.

# **Modelling and Evaluation**

### **KNN Modelling**
"""

# Buat pipeline scaling + model
pipeline_knn = Pipeline([
    ('scaler', StandardScaler()),
    ('knn', KNeighborsClassifier(n_neighbors=5))
])

# Fit pipeline dengan data training
pipeline_knn.fit(X_knn_train, y_knn_train)

# Evaluasi di data test
y_knn_pred = pipeline_knn.predict(X_knn_test)

"""Kode tersebut membuat pipeline machine learning yang terdiri dari dua tahap: pertama, melakukan standarisasi fitur menggunakan StandardScaler agar setiap fitur memiliki skala yang sama, dan kedua, menerapkan algoritma klasifikasi K-Nearest Neighbors (KNN) dengan 5 tetangga terdekat. Pipeline ini kemudian dilatih menggunakan data pelatihan X_knn_train dan y_knn_train, sehingga proses scaling dan pelatihan model dilakukan secara berurutan dan otomatis dalam satu langkah.

### **KNN Evaluation**
"""

# Hitung metrik
acc = accuracy_score(y_knn_test, y_knn_pred)
prec = precision_score(y_knn_test, y_knn_pred)
rec = recall_score(y_knn_test, y_knn_pred)
f1 = f1_score(y_knn_test, y_knn_pred)
cm = confusion_matrix(y_knn_test, y_knn_pred)

print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
print(f"F1-Score : {f1:.4f}")

# Visualisasi confusion matrix dengan heatmap
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted No', 'Predicted Yes'], yticklabels=['Actual No', 'Actual Yes'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

"""Hasil evaluasi model menunjukkan bahwa meskipun **akurasinya cukup tinggi (83,02%), nilai precision (80%) dan terutama recall (10,26%) serta F1-score (18,18%) sangat rendah.** Ini mengindikasikan bahwa model banyak melewatkan (gagal mendeteksi) kasus positif, sehingga performanya buruk dalam mendeteksi kelas minoritas, **meskipun secara keseluruhan tampak akurat karena kemungkinan data tidak seimbang.**

### **Random Forest Modelling**
"""

# Buat model Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
# Train model
rf_model.fit(X_rf_train, y_rf_train)

"""Kode tersebut membuat dan melatih model Random Forest untuk klasifikasi. Pertama, objek model rf_model dibuat menggunakan RandomForestClassifier dari pustaka sklearn.ensemble, dengan parameter **n_estimators=100** yang berarti model akan menggunakan 100 pohon keputusan dalam ensemble-nya, dan **random_state=42** untuk memastikan hasil yang konsisten setiap kali dijalankan. Selanjutnya, model dilatih menggunakan data pelatihan X_rf_train (fitur) dan y_rf_train (label) dengan memanggil metode .fit(), sehingga model belajar pola dari data untuk digunakan dalam prediksi selanjutnya.

### **Random Forest Evaluation**
"""

# Prediksi
y_rf_pred = rf_model.predict(X_rf_test)

# Evaluasi
acc = accuracy_score(y_rf_test, y_rf_pred)
prec = precision_score(y_rf_test, y_rf_pred)
rec = recall_score(y_rf_test, y_rf_pred)
f1 = f1_score(y_rf_test, y_rf_pred)
cm = confusion_matrix(y_rf_test, y_rf_pred)

print("=== Evaluation Metrics ===")
print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
print(f"F1-Score : {f1:.4f}")

# Visualisasi Confusion Matrix
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',
            xticklabels=['Predicted No', 'Predicted Yes'],
            yticklabels=['Actual No', 'Actual Yes'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Random Forest')
plt.show()

"""Hasil evaluasi model Random Forest menunjukkan bahwa **akurasi model cukup tinggi, yaitu 84,43%, dan precision sempurna (1.0000), yang berarti semua prediksi positif benar. Namun, recall sangat rendah (15,38%), menunjukkan bahwa model hanya berhasil menangkap sebagian kecil dari seluruh kasus positif yang sebenarnya ada. F1-score pun rendah (26,67%), mengindikasikan ketidakseimbangan antara precision dan recall.** Ini menunjukkan bahwa meskipun model sangat yakin saat memprediksi positif,**ia sangat jarang melakukannya — kemungkinan besar karena dataset tidak seimbang, sehingga model cenderung mengabaikan kelas minoritas.**"""

joblib.dump(rf_model, 'rf_model.pkl')

"""Setelah membandingkan hasil kedua model, Random Forest dinilai sebagai model yang paling baik berdasarkan metrik evaluasi. Oleh karena itu, model Random Forest tersebut disimpan secara permanen, sehingga dapat dengan mudah digunakan kembali tanpa perlu melatih ulang.

# **Interpretation**
"""

rf_model = joblib.load('rf_model.pkl')
# Pastikan kolom 'Attrition' tidak ada saat prediksi
df_unlabeled = df_unlabeled.drop('Attrition', axis=1)

# Preprocessing manual seperti sebelumnya
df_unlabeled = df_unlabeled.drop(['EmployeeId', 'Over18', 'StandardHours'], axis=1)
df_unlabeled['Gender'] = df_unlabeled['Gender'].map({'Male': 0, 'Female': 1})
df_unlabeled['OverTime'] = df_unlabeled['OverTime'].map({'No': 0, 'Yes': 1})
df_unlabeled = pd.get_dummies(df_unlabeled, columns=['BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus'], drop_first=True)

# Sesuaikan kolom agar sama dengan training data
missing_cols = set(X_rf_train.columns) - set(df_unlabeled.columns)
for c in missing_cols:
    df_unlabeled[c] = 0
df_unlabeled = df_unlabeled[X_rf_train.columns]

# Prediksi probabilitas dan kelas awal
probs = rf_model.predict_proba(df_unlabeled)[:, 1]
initial_preds = rf_model.predict(df_unlabeled)

# Terapkan aturan custom (misal jika prob < 0.5, flip prediksi)
final_preds = []
for pred, prob in zip(initial_preds, probs):
    if prob < 0.5:
        final_preds.append(1 - pred)
    else:
        final_preds.append(pred)

# Buat DataFrame hasil akhir dengan kolom yang diinginkan
result_df = pd.DataFrame({
    'prob_actual': probs,
    'prediksi_awal': initial_preds,
    'prediksi_final': final_preds
})

print(result_df)

"""1. **Preprocessing Manual**  
   Dataset `df_unlabeled` yang sebelumnya telah dipisahkan, terlebih dahulu dilakukan preprocessing secara manual agar fitur-fiturnya siap untuk diprediksi oleh model.

2. **Penerapan Model Random Forest**  
   Setelah preprocessing selesai, dataset tersebut dimasukkan ke model Random Forest yang sudah disimpan sebelumnya untuk melakukan prediksi.

3. **Output Prediksi**  
   Model menghasilkan dua output utama:  
   - **Probabilitas**: Menggambarkan seberapa besar keyakinan model terhadap prediksi kelas tertentu.  
   - **Prediksi awal**: Kelas yang diprediksi model berdasarkan probabilitas terbesar.

4. **Penerapan Aturan pada Prediksi Berdasarkan Probabilitas**  
   - Jika probabilitas prediksi awal **di bawah 0,5**, maka dianggap model kurang yakin dengan prediksi tersebut, sehingga hasil prediksi akan **dibalik** (diubah ke kelas sebaliknya).  
   - Jika probabilitas prediksi awal **di atas 0,5**, maka model dianggap memberikan prediksi yang akurat, sehingga hasil prediksi **dipertahankan** tanpa perubahan.

Dengan aturan ini, prediksi pada `df_unlabeled` menjadi lebih reliable dengan mempertimbangkan tingkat kepercayaan model pada setiap prediksi.
"""

# Ambil nama fitur dari data training
feature_names = X_rf_train.columns

# Ambil nilai feature importance dari model
importances = rf_model.feature_importances_

# Buat DataFrame untuk keperluan visualisasi
feat_imp_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Tampilkan tabel
print(feat_imp_df)

# Visualisasi Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feat_imp_df, palette='viridis')
plt.title('Feature Importance - Random Forest')
plt.xlabel('Importance Score')
plt.ylabel('Features')
plt.tight_layout()
plt.show()

# feat_imp_df.to_csv("feature_importance_attrition.csv", index=False)

